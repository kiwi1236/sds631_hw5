{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0yVeQeuciOs"
      },
      "source": [
        "## Generate data from sparse linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cewUCsuuHQVq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(431)\n",
        "\n",
        "def generate_data(m, n, s, sigma = 0.1):\n",
        "    # Step 1: Generate X from Gaussian distribution\n",
        "    X = np.random.randn(m, n)\n",
        "\n",
        "    # Step 2: Create beta_star with first s entries as 1 and the rest 0\n",
        "    beta_star = np.zeros(n)\n",
        "    beta_star[:s] = 1\n",
        "\n",
        "\n",
        "    y = X @ beta_star + sigma * np.random.randn(m,)\n",
        "\n",
        "    return X, y, beta_star\n",
        "\n",
        "\n",
        "# evaluate the error\n",
        "def l2_error(beta, beta_star):\n",
        "    return np.linalg.norm(beta - beta_star)\n",
        "\n",
        "\n",
        "# # Example usage:\n",
        "# m, n, s = 100, 10, 3  # m samples, n features, s sparsity\n",
        "# X, y, beta_star = generate_data(m, n, s)\n",
        "# X.shape, y.shape, beta_star.shape  # Checking the shapes of the generated data\n",
        "\n",
        "\n",
        "\n",
        "def plot_results(iterates, objective_values, beta_star):\n",
        "    # Calculate the errors between iterates and beta_star\n",
        "    errors = [l2_error(beta_t, beta_star) for beta_t in iterates]\n",
        "\n",
        "    # Create Figure 1: Error vs. Iteration\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(errors)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Error (||beta_t - beta_star||)')\n",
        "    plt.title('Error vs. Iteration')\n",
        "\n",
        "    # Create Figure 2: Function Value vs. Iteration\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(objective_values)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Function Value')\n",
        "    plt.title('Function Value vs. Iteration')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebdE4UtYjLaX"
      },
      "source": [
        "## Generate 200 data points with dimension $n=200$ and sparsity is $s = 5$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO2UeufUjK5U"
      },
      "outputs": [],
      "source": [
        "m, n, s = 200, 100, 5  # m samples, n features, s sparsity\n",
        "X, y, beta_star = generate_data(m, n, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib5CF4xCcohk"
      },
      "source": [
        "## Method 1: Convex optimization via CVX\n",
        "\n",
        "We use CVXPY to solve two convex optimization problems:\n",
        "\n",
        "$\\min \\ell(\\beta) + \\lambda \\cdot \\|\\beta\\|_1 $,\n",
        "\n",
        "$\\min \\ell(\\beta)$ subject to $\\| \\beta \\_1 \\leq \\lambda$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kNOe8HuIRJ3",
        "outputId": "7ecd29e9-ac5d-4082-a835-00b82c347c4e"
      },
      "outputs": [],
      "source": [
        "import cvxpy as cp\n",
        "\n",
        "def solve_cvx_reg(lambda_val, X, y):\n",
        "    m, n = X.shape\n",
        "\n",
        "    beta_cvx = cp.Variable(n)\n",
        "\n",
        "    least_squares = cp.norm2(y - X @ beta_cvx)**2\n",
        "\n",
        "    problem = cp.Problem(cp.Minimize(least_squares + lambda_val * cp.norm(beta_cvx, 1)))\n",
        "\n",
        "    problem.solve()\n",
        "\n",
        "    return beta_cvx.value\n",
        "\n",
        "def solve_cvx_constraint(lambda_val, X, y):\n",
        "    m, n = X.shape\n",
        "\n",
        "    beta_cvx = cp.Variable(n)\n",
        "\n",
        "    least_squares = cp.norm2(y - X @ beta_cvx)**2\n",
        "\n",
        "\n",
        "    objective = cp.Minimize(cp.norm2(y - X @ beta_cvx)**2)\n",
        "    constraints = [cp.norm(beta_cvx, 1) <= lambda_val]\n",
        "    problem = cp.Problem(objective, constraints)\n",
        "    problem.solve()\n",
        "    return beta_cvx.value\n",
        "\n",
        "\n",
        "\n",
        "beta_cvx_con = solve_cvx_constraint(5, X, y)\n",
        "beta_cvx_reg = solve_cvx_reg(6, X, y)\n",
        "\n",
        "print(f\"L1-regularized problem -- Error of Convex optimization from CVXPY: \\n{l2_error(beta_cvx_reg, beta_star)}\\n\")\n",
        "print(f\"L1-constrained problem -- Error of Convex optimization from CVXPY: \\n{l2_error(beta_cvx_con, beta_star)}\")\n",
        "\n",
        "if max(l2_error(beta_cvx_reg, beta_star), l2_error(beta_cvx_con, beta_star)) < 0.05:\n",
        "  print(\"\\nThese two methods recovers the parameter acurrately \\n\")\n",
        "else:\n",
        "  print(\"\\nThe error seems a bit large, perhaps you need to check the code or tune the hyperparameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUoYvdRKdblo"
      },
      "source": [
        "# Method 2: Proximal Gradient Descent\n",
        "\n",
        "The objective function is $\\ell(\\beta) + \\lambda \\| \\beta\\|_1$. The proximal operator reduces to soft-thresholding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "X81L8WTtNgqY",
        "outputId": "5f624716-e27c-4ac9-a4c6-b3d87aaae074"
      },
      "outputs": [],
      "source": [
        "def soft_threshold(v, param):\n",
        "    # Soft_thresholding function\n",
        "    # Your code here\n",
        "    pass\n",
        "\n",
        "\n",
        "def proximal_gradient(beta_0, X, y, alpha, lambda_, N_iter):\n",
        "    # Initialize variables\n",
        "    beta = beta_0\n",
        "    objective_values = []  # To store the sequence of objective values\n",
        "    iterates = []  # To store the sequence of iterates (beta values)\n",
        "\n",
        "    for iteration in range(N_iter):\n",
        "        # Compute the gradient of the objective function\n",
        "        gradient = -2 * X.T.dot(y - X.dot(beta))\n",
        "\n",
        "        # Update beta using soft-thresholding\n",
        "        beta = soft_threshold(beta - alpha * gradient, alpha * lambda_)\n",
        "\n",
        "        # Calculate the objective value and append to the list\n",
        "        objective_value = np.linalg.norm(y - X.dot(beta))**2 + lambda_ * np.linalg.norm(beta, 1)\n",
        "        objective_values.append(objective_value)\n",
        "\n",
        "        # Append the current iterate (beta) to the list\n",
        "        iterates.append(beta)\n",
        "\n",
        "    return beta, iterates, objective_values\n",
        "\n",
        "\n",
        "## Now let's test proximal gradient\n",
        "\n",
        "# initialization\n",
        "beta_init = 10 * np.ones(n)\n",
        "lambda_prox = 3\n",
        "stepsize = 0.001\n",
        "N_iter = 200\n",
        "\n",
        "beta_prox, beta_prox_seq, fun_prox_seq = proximal_gradient(beta_init, X, y, stepsize, lambda_prox, N_iter)\n",
        "\n",
        "print(f\"L1-regularized problem -- Error of Proximal gradient: \\n{l2_error(beta_prox, beta_star)}\\n\")\n",
        "\n",
        "if l2_error(beta_prox, beta_star) < 0.05:\n",
        "  print(\"\\nProximal Gradient recovers the parameter acurrately \\n\")\n",
        "else:\n",
        "  print(\"\\nThe error seems a bit large, perhaps you need to check the code or tune the hyperparameters\")\n",
        "\n",
        "# generate plots\n",
        "plot_results(beta_prox_seq, fun_prox_seq, beta_star)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYTuBYD2iXzJ"
      },
      "source": [
        "# Method 3: Projected Gradient Descent\n",
        "\n",
        "The optimization problem is $\\min \\ell(\\beta)$ subject to $\\| \\beta \\|_0 \\leq t$ for some integer $t$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "gDRQVSFMh1gy",
        "outputId": "3f569c48-941b-4033-ce89-1f42f6e331f0"
      },
      "outputs": [],
      "source": [
        "def project_largest_t_elements(beta, t):\n",
        "    # Your code here\n",
        "\n",
        "\n",
        "\n",
        "def projected_gradient(beta_0, X, y, alpha, t, N_iter):\n",
        "    # Initialize variables\n",
        "    beta = beta_0\n",
        "    objective_values = []  # To store the sequence of objective values\n",
        "    iterates = []  # To store the sequence of iterates (beta values)\n",
        "\n",
        "    for iteration in range(N_iter):\n",
        "        # Compute the gradient of the objective function\n",
        "        gradient = -2 * X.T.dot(y - X.dot(beta))\n",
        "\n",
        "        # Update beta using soft-thresholding\n",
        "        beta = project_largest_t_elements(beta - alpha * gradient, t)\n",
        "        # print(beta)\n",
        "        # Calculate the objective value and append to the list\n",
        "        objective_value = np.linalg.norm(y - X.dot(beta))**2\n",
        "        objective_values.append(objective_value)\n",
        "\n",
        "        # Append the current iterate (beta) to the list\n",
        "        iterates.append(beta)\n",
        "\n",
        "    return beta, iterates, objective_values\n",
        "\n",
        "\n",
        "\n",
        "## Now let's test proximal gradient\n",
        "\n",
        "# initialization\n",
        "beta_init = 10 * np.ones(n)\n",
        "t = 10\n",
        "stepsize = 0.001\n",
        "N_iter = 200\n",
        "\n",
        "beta_proj, beta_proj_seq, fun_proj_seq = projected_gradient(beta_init, X, y, stepsize, t, N_iter)\n",
        "\n",
        "print(f\"L0-constrained problem -- Error of Projected gradient: \\n{l2_error(beta_proj, beta_star)}\\n\")\n",
        "\n",
        "if l2_error(beta_proj, beta_star) < 0.05:\n",
        "  print(\"\\nProjected Gradient recovers the parameter acurrately \\n\")\n",
        "else:\n",
        "  print(\"\\nThe error seems a bit large, perhaps you need to check the code or tune the hyperparameters\")\n",
        "\n",
        "# generate plots\n",
        "plot_results(beta_proj_seq, fun_proj_seq, beta_star)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnkeMQrWkBpX"
      },
      "source": [
        "# Method 4: Frank-Wolfe Method\n",
        "\n",
        "The optimization problem is $\\min \\ell(\\beta)$ subject to $\\| \\beta \\|_1 \\leq \\lambda$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "AmDb1QSQj3ES",
        "outputId": "1e9ac913-d7b9-4d34-986f-da8279d9c32b"
      },
      "outputs": [],
      "source": [
        "def update_direction(grad, lambda_):\n",
        "    # Your code here\n",
        "    pass\n",
        "\n",
        "\n",
        "def frank_wolfe(beta_0, X, y, alpha, lambda_, N_iter):\n",
        "    # Initialize variables\n",
        "    beta = beta_0\n",
        "    objective_values = []  # To store the sequence of objective values\n",
        "    iterates = []  # To store the sequence of iterates (beta values)\n",
        "\n",
        "    for iteration in range(N_iter):\n",
        "        # Compute the gradient of the objective function\n",
        "        gradient = -2 * X.T.dot(y - X.dot(beta))\n",
        "\n",
        "        # Update direction\n",
        "        direction = update_direction(gradient, lambda_)\n",
        "\n",
        "        beta = (1-alpha) * beta + alpha * direction\n",
        "\n",
        "        # print(beta)\n",
        "        # Calculate the objective value and append to the list\n",
        "        objective_value = np.linalg.norm(y - X.dot(beta))**2\n",
        "        objective_values.append(objective_value)\n",
        "\n",
        "        # Append the current iterate (beta) to the list\n",
        "        iterates.append(beta)\n",
        "\n",
        "    return beta, iterates, objective_values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Now let's test Frank-Wolfe\n",
        "\n",
        "# initialization\n",
        "beta_init = 10 * np.zeros(n)\n",
        "t = 6\n",
        "stepsize = 0.005\n",
        "N_iter = 500\n",
        "\n",
        "beta_fw, beta_fw_seq, fun_fw_seq = frank_wolfe(beta_init, X, y, stepsize, t, N_iter)\n",
        "\n",
        "print(f\"L1-constrained problem -- Error of Frank-Wolfe: \\n{l2_error(beta_fw, beta_star)}\\n\")\n",
        "\n",
        "if l2_error(beta_fw, beta_star) < 0.05:\n",
        "  print(\"\\nFrank-Wolfe recovers the parameter acurrately \\n\")\n",
        "else:\n",
        "  print(\"\\nThe error seems a bit large, perhaps you need to check the code or tune the hyperparameters\")\n",
        "\n",
        "# generate plots\n",
        "plot_results(beta_fw_seq, fun_fw_seq, beta_star)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skyhn6u7nn-N"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
